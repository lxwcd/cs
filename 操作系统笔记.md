操作系统学习笔记  
    
# 学习资源  
> [深入理解计算机系统 第三版](https://pan.baidu.com/s/1j6u_txqDCMLX-jCdnRdbOw?pwd=oqon)  
> [现代操作系统 第四版](https://pan.baidu.com/s/1JSv_M4K9R3nmTVdGOqSAtQ?pwd=jsnx)  
> [图解系统](https://xiaolincoding.com/os/)  
    
# 线路位宽和 CPU 位宽  
> [线路位宽与 CPU 位宽](https://xiaolincoding.com/os/1_hardware/how_cpu_run.html#线路位宽与-cpu-位宽)  
    
1. 线路位宽（Bus Width）  
线路位宽是指计算机系统中数据传输通道的宽度，即一次可以传输的数据位数。  
它通常用于描述内部数据传输的带宽和数据通路的宽度。  
例如，系统的数据总线和地址总线的宽度可以被称为线路位宽。  
线路位宽的大小影响着系统的数据传输速度和处理能力。  
    
2. CPU位宽（CPU Word Size）  
CPU位宽是指CPU寄存器中一次可以处理的数据位数。  
它表示CPU内部的数据处理能力和寄存器的大小。  
常见的CPU位宽有8位、16位、32位和64位等。  
较大的CPU位宽通常意味着CPU能够一次处理更多的数据，提供更高的计算性能和数据吞吐量。  
    
虽然线路位宽和CPU位宽都与计算机系统中的数据处理和传输有关，但它们描述的是不同的方面。  
线路位宽主要关注数据传输通道的宽度和带宽，而CPU位宽则表示CPU的内部数据处理能力。  
它们在系统设计和性能评估中都扮演着重要的角色。  
    
当我们考虑一个计算机系统的线路位宽和CPU位宽时，我们可以通过以下示例来说明它们的区别：  
假设我们有一台计算机系统，其中的数据总线和地址总线的线路位宽为32位，而CPU的位宽为64位。  
    
对于线路位宽（32位）：  
- 这意味着系统的数据总线和地址总线每次可以传输32位的数据。  
- 内存访问时，每个内存地址的数据在一次传输中最多可以传输32位的数据，即 4G 大小的数据。  
- 当CPU要从内存中读取数据时，它可以一次性读取32位的数据，而不需要多次传输。  
    
对于CPU位宽（64位）：  
- 这意味着CPU的寄存器和数据通路可以同时处理64位的数据。  
- CPU内部的算术逻辑单元（ALU）可以一次性执行64位的算术和逻辑操作。  
- 当CPU从内存中读取数据时，它可以一次性读取64位的数据到寄存器中，从而进行更高精度的计算。  
    
综上所述：  
- 线路位宽（32位）描述了数据总线和地址总线的传输能力，即每次可以传输的数据位数。  
- CPU位宽（64位）描述了CPU内部的寄存器和数据通路的能力，即一次可以处理的数据位数。  
- CPU位宽最好不小于线路位宽  
    
# 缓存管理  
> [CSAPP-第六章-Cache Memories](https://github.com/lxwcd/cs/blob/main/csapp/notes/深入理解计算机系统——第六章%20The%20Memory%20Hierarchy.md)  
> [Cache Memory](https://www.geeksforgeeks.org/cache-memory/)  
  
尽管缓存是一个通用的概念，但当我们在讨论计算机系统中的缓存（cache）时，通常指的是存在于CPU中的cache。这种缓存利用了局部性原理，存储了最近访问的数据和指令，以减少CPU访问内存的次数。因为CPU中的寄存器访问速度非常快，但容量有限；而内存的容量虽然大，但访问速度远慢于CPU寄存器。CPU cache的存在有效地弥补了寄存器文件和内存之间访问速度的差异，从而提高了程序的执行效率。CPU cache通常分为几级（比如L1、L2、L3），越接近CPU的级别（例如L1）速度越快但容量越小，越远离CPU的级别（例如L3）则相反。  
  
CPU 中的 cache (L1, L2, L3) 大小通常为 KB 到 几MB。  
  
CPU cache 可以利用局部性原理提高程序的执行效率。  
  
CPU缓存的管理主要由硬件的缓存控制器进行管理。缓存快的大小（缓存块大小），存储算法等参数通常在CPU设计时就已经决定，并且嵌入在处理器的微架构中。  
- 缓存块的大小是由CPU的设计者根据目标应用程序的局部性特征以及制造成本和效能之间的权衡来确定的。  
- 存储算法，包括替换算法（如LRU——最近最少使用，FIFO——先入先出，随机替换等）和映射策略（如直接映射，组相联映射，全相联映射）  
- 这些也是由CPU设计者根据预期的使用场景来选择和实现的。  
缓存的大小、替代策略、写策略（写回或写穿）等参数直接影响缓存的性能，因此都是CPU设计和性能优化中重要的考量因素。  
  
缓存映射策略，如直接映射缓存，全相联缓存等也是硬件管理的，在CPU设计阶段，会确定包括直接映射缓存、全相联缓存等缓存映射策略。直接映射缓存是一种简单的映射策略，它是指每个主存块只能映射到缓存中的一个特定的缓存行。全相联缓存则是最灵活的映射策略，任何主存块都可以映射到缓存中的任何位置。这些设计选择涉及到缓存的结构和如何映射内存数据到缓存中，它们需要在CPU设计时就被确定下来，并在硬件层面上实现。用户一般无法更改这些硬件层面的设计。  
  
硬件决定了内存中的数据块如何映射到缓存中——这是通过缓存的映射策略（如直接映射、全相联映射、组相联映射）来实现的。这个映射策略定义了内存中的哪些数据可以放在缓存的哪个位置。  
  
## 缓存中的写策略  
> [CSAPP-第六章-issues with writes](https://github.com/lxwcd/cs/blob/main/csapp/notes/深入理解计算机系统——第六章%20The%20Memory%20Hierarchy.md#645-issues-with-writes)  
> [Write Through and Write Back in Cache](https://www.geeksforgeeks.org/write-through-and-write-back-in-cache/)  
  
CPU 向缓存中写数据时，如果缓存命中，那么缓存更新后，有两种处理方式：write-through 和 write-back。  
如果缓存中没有数据，有两种处理方式：write-allocate 和 no-write-allocate。  
  
**通常的方式是**：  
- Write-through + No-write-allocate  
- Write-back + Write-allocate (更常用)  
  
### **Write-through**  
Write-through 策略下提到的缓存位于 CPU 中。当 CPU 向这个缓存中写入数据时，数据会同时被写入到更慢的存储，即主存中，以确保两者之间的数据一致性。  
    
将更新后的 w **立刻**写到下一个低等级的块中（前面讲过，存储器的层次等级，当前命中的缓存保存的是它第一等级的部分数据的副本），这样很**费时**，因为访问低层次的时间更长。  
    
- **优点**：  
  - 简单易实现，在数据写入操作中保持高级存储（如主存）和缓存之间的一致性。  
  - 数据安全性较高，因为每次写操作都同步更新到主存，减少数据丢失的风险。  
- **缺点**：  
  - 性能开销较大，因为每次写操作都需要访问慢速的主存，这会降低整体的系统性能。  
  - 增加了总线和存储设备的负载，因为每次写操作都涉及到主存。  
      
### **Write-back**  
w 先保存在当前缓存中，只有**当前块**的内容要被**覆盖**时才将数据写回到**低等级块**中。这样做的**缺点**是需要额外的 **dirty-bit** 来表示该缓存块是否被修改。  
  
当脏数据对于的块要被新的缓存替换时，则将其数据更新到内存中。在某些缓存映射策略中，可能涉及到内存淘汰算法，选择淘汰哪个脏数据。  
  
在传统的缓存管理中，可能存在这样一种情况，即当多个内存块映射到同一缓存行或位置时，后来的内存块会覆盖先前的数据。这种情况通常发生在直接映射缓存中，其中每个内存块只有一个确定的缓存位置可用。当另一个需要映射到同一位置的块被加载时，就会替换掉当前缓存中的块。  
  
然而，在更复杂的缓存架构中，如组相联映射或全相联映射，缓存替换策略（或淘汰算法）就显得尤为重要。在这些情况下，硬件不仅仅是简单地用新的内存块覆盖旧的内存块。相反，它将使用特定的算法（如最近最少使用LRU或者其他策略）来决定哪个内存块应该被替换。  
  
在组相联或全相联映射的缓存中，每个内存块可以有多个可能的缓存位置。淘汰算法如LRU策略，会根据数据的使用情况选择最佳的块来替换，从而提高缓存的命中率和性能。这意味着并不总是最后到来的内存块覆盖之前的缓存，而是淘汰算法决定的那个块被替换。  
    
- **优点**：  
  - 提高系统性能，因为写操作仅在缓存中进行，减少了对慢速主存的访问。  
  - 减少了总线和主存的负载，因为数据不会立即写入主存。  
- **缺点**：  
  - 需要更复杂的逻辑来维护缓存和主存之间的一致性，尤其是在多核心系统中。  
  - 数据安全性降低，因为数据仅存在于缓存中时，有丢失的风险。  
  
### **Write-allocate**  
找到要写入的数据块放到缓存中，更新缓存的块。缺点是这样每次未命中都要额外花时间将数据写到缓存中，这种方式是考虑到 spacial locality，可能接下来用到的数据已经在缓存中，能命中了。  
      
### **No-write-allocate**  
直接将数据写到低层次的块中，不放入缓存中。  
  
## L1 和 L2 缓存在不同 CPU core 中一致性  
> [CSAPP-第六章-Cache Hierarchy](https://github.com/lxwcd/cs/blob/main/csapp/notes/深入理解计算机系统——第六章%20The%20Memory%20Hierarchy.md#646-anatomy-of-a-real-cache-hierarchy)  
  
L1和L2缓存通常是每个CPU核心独占的，这样设计主要是为了减少访问延迟和增加处理速度。然而，这种缓存的分配策略也会带来一些问题：  
  
1. 缓存一致性（Coherency）问题：当多个处理器核心各自有独立的L1和L2缓存时，如果核心间需要共享数据，就需要一个机制来保证所有核心所看到的数据是一致的。例如，如果核心A更新了一个数据项，而这个数据项的旧值还存储在核心B的L1或L2缓存中，那么核心B可能就会使用错误的旧值。为解决这个问题，多核处理器通常会实施一个称为缓存一致性协议的机制，比如MESI（Modify, Exclusive, Shared, Invalid）协议，以确保所有核心中缓存的数据保持一致。  
  
2. 空间效率问题：每个CPU核心都有自己的L1和L2缓存可能会导致重复存储相同的数据。假设一个运行在核心A上的线程正在访问一个数据集，而这个数据集同样也被核心B上的线程访问，那么这个数据集就可能会在两个核心的L1或L2缓存中各有一份拷贝，这显然不是空间使用上的最佳方式。  
  
3. 任务切换影响：当操作系统进行任务切换时，新的任务可能不得不使用并更新L1和L2缓存中的数据。如果切换频繁，那么缓存可能会频繁地装入新数据，从而降低了缓存的有效性。  
  
4. 温度管理：由于每个CPU核心都有自己的缓存，CPU的每个核心都会产生热量。核心的数量越多，和每个核心上的缓存尺寸越大，散热就成了更大的挑战。  
  
5. 不对称访问延时：在多核心处理器中，通常各个核心离L3共享缓存的物理距离不同，导致访问共享缓存的时间差异，但在L1和L2级别，由于是核心独占的，不会出现这种情况。但这意味着数据不得不经常从更慢的L3缓存中重新装入L1和L2缓存，这同样会引入延迟。  
  
总之，虽然L1和L2缓存提供了更快的访问速度和较低的延迟，但它们也引入了一系列关于一致性、空间效率、任务切换、温度管理和访问延时的问题。这些挑战需要通过缓存一致性协议、智能的任务调度以及高效的散热解决方案来克服。  
  
这个问题类型有状态应用中进行负载均衡时，如几个 nginx 作为 web server 时需要做会话一致性。  
  
### MESI 缓存一致性协议  
MESI缓存一致性协议通过以下几个状态来确保数据在多个缓存之间保持一致性：  
  
1. Modified（M）状态：表示该缓存行是脏的，即它被修改过，但还没有写回到主存储器。此时，该缓存行在其他任何缓存中都不能有有效的副本。  
  
2. Exclusive（E）状态：表示该缓存行可能被修改，它是干净的（与主存中的内容相同），并且在其他任何缓存中都没有这个数据的副本。  
  
3. Shared（S）状态：表示该缓存行是干净的，并且可能在其他缓存中也有相同数据的副本。  
  
4. Invalid（I）状态：表示该缓存行是无效的，里面的数据不被缓存使用。  
  
当CPU写入数据时，协议会通过以下机制来维护一致性：  
  
- 如果写入的缓存行目前处于共享状态，那么缓存控制逻辑会向其他CPU发出一个失效信号，其它CPU的缓存必须把对应的缓存行设为无效状态。在此之后，写操作CPU的缓存行状态变为Modified。  
  
- 如果写入的缓存行目前处于独占状态，那么缓存控制逻辑无需通知其他CPU，直接将缓存行状态改为Modified。  
  
通过这种方式，MESI协议保证了当一个核心更新数据时，其他核心能够看到最新的数据，或者被迫重新从内存中加载数据，从而确保了数据的一致性。  
  
### CPU 亲缘性  
通过绑定CPU核心（CPU亲缘性）可以在一定程度上帮助避免L1和L2缓存的不一致性问题。CPU亲缘性是指将某些任务或线程绑定到某个特定的CPU核心上运行，这样可以确保任务在一个固定的处理器上执行，减少了数据在不同处理器核心间的移动，因此可以减少不同CPU核心间L1和L2缓存的不一致性问题。  
  
当一个线程只在一个核心上运行时，它所需的数据将更多地留在该核心的L1和L2缓存中，其他核心不会存有该数据的副本，从而减少了缓存一致性的开销。这种方法常用于性能敏感的应用程序，可以最小化缓存失效，并提高缓存利用率。  
  
然而，需要注意的是，这不能完全替代MESI之类的缓存一致性协议。在多个核心需要访问和修改同一数据的情况下，仍然需要通过一致性协议来确保数据的正确性。此外，过分依赖CPU亲缘性也可能导致处理器资源的不平衡使用，因此在使用CPU亲缘性时需要仔细考虑调度和负载均衡。  
  
## 缓存关联性的影响  
缓存映射策略主要有三种：直接映射、组关联（Set-Associative）和全关联（Fully-Associative）。这些策略之间的主要区别在于它们如何决定一个内存块映射到缓存中的哪个位置。  
  
1. **直接映射**：每个内存块只能映射到缓存中的一个特定位置。这种映射方法实现简单，但可能导致较高的缓存冲突率，特别是当多个频繁使用的内存块映射到同一缓存行时。  
  
2. **全关联**：内存块可以映射到缓存中的任何位置。全关联映射提供了最大的灵活性，理论上可以获得最高的缓存命中率，但实现成本高，因为每次缓存查找都需要搜索整个缓存。  
  
3. **组关联**：这是一种折中方案，缓存被分为多个组，每个内存块可以映射到一个组中的任何位置，但不能映射到其他组。组关联结合了直接映射的简单性和全关联的高缓存命中率。组关联性的提高通常能增加缓存命中率，因为它减少了冲突缓存。  
  
关联性的提高（从直接映射到组关联，再到全关联）通常会增加缓存命中率，因为它减少了缓存冲突，允许一个内存块有更多的缓存位置可供映射。在组关联或全关联缓存中，一个内存块能映射到几个缓存位置的情况主要是为了减少缓存冲突和提高缓存利用率。  
  
通常的做法：更高层次相关性较低，低层次缓存相关性更高（为了增加 hit rate），如前面图 6.39 所示， L3 是 16-way，L1 和 L2 是 8-way。  
  
引用的内容中没有提及关联性如何影响缓存命中率的具体信息。不过，我可以基于我所掌握的知识来回答这个问题。  
  
关联性在缓存设计中是指一给定的内存块可以被加载到缓存中的哪些位置。一般来说，关联性高指的是一个内存块有多个位置可供选择，而关联性低则意味着内存块的可选位置较少。  
  
# 阻塞 非阻塞 同步 异步  
> UNIX 网络编程 卷1 第三版 第六章  
> 和书中讲的一样：[阻塞与非阻塞 I/O VS 同步与异步 I/O](https://www.xiaolincoding.com/os/6_file_system/file_system.html#阻塞与非阻塞-i-o-vs-同步与异步-i-o)  
> 包含书中的示意图：[I / O MULTIPLEXING : THE Select AND Poll FUNCTIONS](https://www.brainkart.com/article/I-O-Multiplexing---the-Select-and-Poll-Functions_9113/)  
    
书中五种 I/O 模型：  
    
1. Blocking I/O model  
用户程序执行系统调用后一直等待内核返回结果，此时进程处于阻塞状态  
内核经过两个过程：准备数据和将数据从内核空间拷贝到用户空间  
    
2. Nonblocking I/O model  
用户程序执行系统调用，如果此时内核还未准备好数据，则返回，然后用户程序一直轮询执行系统调用，直到内核将数据准备好  
在内核准备数据的过程种，用户程序没有阻塞  
在内核将数据从内核空间拷贝到用户空间时，用户程序处于阻塞状态，等待返回结果  
    
5. Asynchronous I/O model  
用户执行系统调用后立即返回，然后用户程序继续执行其他操作，内核异步的准备数据然后拷贝数据，拷贝完成后发送一个信号通知用户程序  
该过程全程未阻塞进行  
    
    
同步（synchronous）和异步（asynchronous）：  
模型 1 和 2 都是同步操作，因为用户程序都要等待结果返回才能继续执行  
A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes.  
    
模型 5 是异步操作，用户程序不用等待结果，不会阻塞  
An asynchronous I/O operation does not cause the requesting process to be blocked.  
    
实际应用举例：  
redis 中 save 命令是同步保存 rdb 文件，会阻塞直到文件创建完成  
redis 中 bgsave 命令则是异步保存 rdb 文件，主进程会 fork 一个子进程来保存文件，主进程不等待而是立即返回后执行其他命令  
    
# I/O 多路复用：select/poll/epoll  
> [9.2 I/O 多路复用：select/poll/epoll](https://xiaolincoding.com/os/8_network_system/selete_poll_epoll.html#最基本的-socket-模型)  
> CSAPP: [12.2 Concurrent Programming with I/O Multiplexing](https://github.com/lxwcd/cs/blob/main/csapp/notes/深入理解计算机系统——第十二章%20Concurrent%20Programming.md#122-concurrent-programming-with-io-multiplexing)  
    
    
# 零拷贝  
> [9.1 什么是零拷贝？](https://xiaolincoding.com/os/8_network_system/zero_copy.html#为什么要有-dma-技术)  
    
    
传统文件 I/O 要将数据先拷贝到内核空间，再从内核空间拷贝到用户空间的原因：  
1. **内存保护和隔离**：  
- 内核空间和用户空间是分开的，有不同的地址空间。内核空间包含操作系统的核心代码和数据，而用户空间包含应用程序的代码和数据。为了保护操作系统免受应用程序的非法访问和损害，内核和用户空间必须进行隔离。  
- 当文件I/O发生时，数据首先存储在内核缓冲区中，这是在内核空间中的缓冲区。然后，通过系统调用，内核将数据从内核缓冲区复制到用户空间的缓冲区。  
    
2. **权限控制**：  
- 内核空间拥有更高的权限，可以直接访问系统资源。为了防止用户空间的应用程序直接访问内核空间的数据，数据的复制是必需的。只有经过内核的授权和验证，数据才会被复制到用户空间。  
    
3. **内核空间管理**：  
- 内核负责管理系统资源，包括处理文件I/O操作。内核在内核空间中维护了文件系统缓存、文件描述符等数据结构，以便高效地进行文件操作。  
- 将数据首先复制到内核空间允许内核使用自己的数据结构和缓存机制，从而提高文件I/O的性能。  
- 内核缓冲区还可以对数据进行预读，以后用户访问的数据可以先从内核缓冲区中查找，如果缓存命中率高，则提高文件I/O的效率。  
    
4. **数据一致性和可靠性**：  
- 数据在内核空间的缓冲区中进行处理，确保了在复制到用户空间之前，数据经历了内核的处理，例如缓存写回、同步等，从而提高了数据的一致性和可靠性。  
    
****************************  
    
直接 I/O 存在的原因：  
- 对于一些大文件，用传统的缓存 I/O 方式，缓存命中率可能不高，反而会因为需要两次数据拷贝而增加开销，降低 I/O 的效率。  
- 直接 I/O 对于需要大规模数据传输的应用非常有用，例如大文件的读写、数据库操作等。在这些场景下，减少数据拷贝的开销可以显著提高性能。  
- 直接 I/O 在需要高性能计算和大数据处理的领域得到广泛应用。这些场景对 I/O 性能的要求非常高，而直接 I/O 提供了更高效的数据传输方式。  
- 直接 I/O 可以降低内存占用，因为数据不需要首先缓存在内核缓冲区中。对于大文件或大量数据的操作，这可以减少内存的使用。  
- 直接 I/O 不依赖于文件系统缓存，因此可以避免一些与缓存相关的问题，如缓存污染或不一致性。  
- 异步 I/O 只支持直接 I/O。  
    
# 内存分配算法  
> [Partition Allocation Methods in Memory Management](https://www.geeksforgeeks.org/partition-allocation-methods-in-memory-management/)  
> [OS Memory Allocation Q & A #2](https://www.tutorialspoint.com/operating_system/os_memory_allocation_qa2.htm)  
    
    
内存分配算法是操作系统中用于管理和分配可用内存空间的策略。以下是几种常见的内存分配算法：  
    
1. 首次适应算法（First Fit Algorithm）：从内存空闲链表的起始位置开始查找，找到第一个能够满足需求的空闲块，并将进程分配到该块。  
    
2. 最佳适应算法（Best Fit Algorithm）：在内存空闲链表中查找最小且能够满足需求的空闲块，并将进程分配到该块。该算法可以最小化内存碎片的产生，但搜索过程较慢。  
    
3. 最坏适应算法（Worst Fit Algorithm）：在内存空闲链表中查找最大的空闲块，并将进程分配到该块。该算法可以减少外部碎片，但可能导致内存利用率降低。  
    
4. 循环首次适应算法（Next Fit Algorithm）：类似于首次适应算法，但从上一次分配的位置开始查找，避免每次都从头开始搜索。  
    
5. 分区算法（Partitioning Algorithms）：将内存划分为固定大小的分区，每个分区可以容纳一个进程。分区算法包括等大小分区算法（Fixed-size Partitioning）和可变大小分区算法（Variable-size Partitioning），如动态分区分配算法（Dynamic Partitioning）和伙伴系统算法（Buddy System）等。  
    
伙伴系统是一种常用的内存分配算法，用于动态分配可变大小的内存块。它通过将可用内存空间划分为大小为2的幂次的块，以满足不同大小的内存需求，并通过合并和拆分操作来管理内存块。  
    
下面是伙伴系统内存分配算法的基本原理：  
    
- 初始化内存：首先，将整个可用的内存空间划分为最大块大小的内存块。通常情况下，内存大小是2的幂次，比如1KB、2KB、4KB等。  
    
- 分配内存：当有进程需要分配内存时，伙伴系统会根据进程的内存需求，找到最小的合适内存块。如果找到的内存块大小正好满足需求，则直接分配给进程。否则，会将该内存块一分为二，形成两个相等大小的伙伴块。  
    
- 合并操作：当进程释放内存时，伙伴系统会检查该内存块的伙伴块是否也是空闲的。如果是，则将两个伙伴块合并成一个更大的内存块，并递归地继续合并相邻的空闲块，直到无法再合并为止。  
