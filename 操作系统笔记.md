操作系统学习笔记  
  
# 学习资源  
> [深入理解计算机系统 第三版](https://pan.baidu.com/s/1j6u_txqDCMLX-jCdnRdbOw?pwd=oqon)  
> [现代操作系统 第四版](https://pan.baidu.com/s/1JSv_M4K9R3nmTVdGOqSAtQ?pwd=jsnx)  
> [图解系统](https://xiaolincoding.com/os/)  
  
# 线路位宽和 CPU 位宽  
> [线路位宽与 CPU 位宽](https://xiaolincoding.com/os/1_hardware/how_cpu_run.html#线路位宽与-cpu-位宽)  
  
1. 线路位宽（Bus Width）  
线路位宽是指计算机系统中数据传输通道的宽度，即一次可以传输的数据位数。  
它通常用于描述内部数据传输的带宽和数据通路的宽度。  
例如，系统的数据总线和地址总线的宽度可以被称为线路位宽。  
线路位宽的大小影响着系统的数据传输速度和处理能力。  
  
2. CPU位宽（CPU Word Size）  
CPU位宽是指CPU寄存器中一次可以处理的数据位数。  
它表示CPU内部的数据处理能力和寄存器的大小。  
常见的CPU位宽有8位、16位、32位和64位等。  
较大的CPU位宽通常意味着CPU能够一次处理更多的数据，提供更高的计算性能和数据吞吐量。  
  
虽然线路位宽和CPU位宽都与计算机系统中的数据处理和传输有关，但它们描述的是不同的方面。  
线路位宽主要关注数据传输通道的宽度和带宽，而CPU位宽则表示CPU的内部数据处理能力。  
它们在系统设计和性能评估中都扮演着重要的角色。  
  
当我们考虑一个计算机系统的线路位宽和CPU位宽时，我们可以通过以下示例来说明它们的区别：  
假设我们有一台计算机系统，其中的数据总线和地址总线的线路位宽为32位，而CPU的位宽为64位。  
  
对于线路位宽（32位）：  
- 这意味着系统的数据总线和地址总线每次可以传输32位的数据。  
- 内存访问时，每个内存地址的数据在一次传输中最多可以传输32位的数据，即 4G 大小的数据。  
- 当CPU要从内存中读取数据时，它可以一次性读取32位的数据，而不需要多次传输。  
  
对于CPU位宽（64位）：  
- 这意味着CPU的寄存器和数据通路可以同时处理64位的数据。  
- CPU内部的算术逻辑单元（ALU）可以一次性执行64位的算术和逻辑操作。  
- 当CPU从内存中读取数据时，它可以一次性读取64位的数据到寄存器中，从而进行更高精度的计算。  
  
综上所述：  
- 线路位宽（32位）描述了数据总线和地址总线的传输能力，即每次可以传输的数据位数。  
- CPU位宽（64位）描述了CPU内部的寄存器和数据通路的能力，即一次可以处理的数据位数。  
- CPU位宽最好不小于线路位宽  
  
# 阻塞 非阻塞 同步 异步  
> UNIX 网络编程 卷1 第三版 第六章  
> 和书中讲的一样：[阻塞与非阻塞 I/O VS 同步与异步 I/O](https://www.xiaolincoding.com/os/6_file_system/file_system.html#阻塞与非阻塞-i-o-vs-同步与异步-i-o)  
> 包含书中的示意图：[I / O MULTIPLEXING : THE Select AND Poll FUNCTIONS](https://www.brainkart.com/article/I-O-Multiplexing---the-Select-and-Poll-Functions_9113/)  
  
  
书中五种 I/O 模型：  
  
1. Blocking I/O model  
用户程序执行系统调用后一直等待内核返回结果，此时进程处于阻塞状态  
内核经过两个过程：准备数据和将数据从内核空间拷贝到用户空间  
  
2. Nonblocking I/O model  
用户程序执行系统调用，如果此时内核还未准备好数据，则返回，然后用户程序一直轮询执行系统调用，直到内核将数据准备好  
在内核准备数据的过程种，用户程序没有阻塞  
在内核将数据从内核空间拷贝到用户空间时，用户程序处于阻塞状态，等待返回结果  
  
5. Asynchronous I/O model  
用户执行系统调用后立即返回，然后用户程序继续执行其他操作，内核异步的准备数据然后拷贝数据，拷贝完成后发送一个信号通知用户程序  
该过程全程未阻塞进行  
  
  
同步（synchronous）和异步（asynchronous）：  
模型 1 和 2 都是同步操作，因为用户程序都要等待结果返回才能继续执行  
A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes.  
  
模型 5 是异步操作，用户程序不用等待结果，不会阻塞  
An asynchronous I/O operation does not cause the requesting process to be blocked.  
  
实际应用举例：  
redis 中 save 命令是同步保存 rdb 文件，会阻塞直到文件创建完成  
redis 中 bgsave 命令则是异步保存 rdb 文件，主进程会 fork 一个子进程来保存文件，主进程不等待而是立即返回后执行其他命令  
  
# I/O 多路复用：select/poll/epoll  
> [9.2 I/O 多路复用：select/poll/epoll](https://xiaolincoding.com/os/8_network_system/selete_poll_epoll.html#最基本的-socket-模型)  
> CSAPP: [12.2 Concurrent Programming with I/O Multiplexing](https://github.com/lxwcd/cs/blob/main/csapp/notes/深入理解计算机系统——第十二章%20Concurrent%20Programming.md#122-concurrent-programming-with-io-multiplexing)  
  
  
# 零拷贝  
> [9.1 什么是零拷贝？](https://xiaolincoding.com/os/8_network_system/zero_copy.html#为什么要有-dma-技术)  
  
  
传统文件 I/O 要将数据先拷贝到内核空间，再从内核空间拷贝到用户空间的原因：  
1. **内存保护和隔离**：  
- 内核空间和用户空间是分开的，有不同的地址空间。内核空间包含操作系统的核心代码和数据，而用户空间包含应用程序的代码和数据。为了保护操作系统免受应用程序的非法访问和损害，内核和用户空间必须进行隔离。  
- 当文件I/O发生时，数据首先存储在内核缓冲区中，这是在内核空间中的缓冲区。然后，通过系统调用，内核将数据从内核缓冲区复制到用户空间的缓冲区。  
  
2. **权限控制**：  
- 内核空间拥有更高的权限，可以直接访问系统资源。为了防止用户空间的应用程序直接访问内核空间的数据，数据的复制是必需的。只有经过内核的授权和验证，数据才会被复制到用户空间。  
  
3. **内核空间管理**：  
- 内核负责管理系统资源，包括处理文件I/O操作。内核在内核空间中维护了文件系统缓存、文件描述符等数据结构，以便高效地进行文件操作。  
- 将数据首先复制到内核空间允许内核使用自己的数据结构和缓存机制，从而提高文件I/O的性能。  
- 内核缓冲区还可以对数据进行预读，以后用户访问的数据可以先从内核缓冲区中查找，如果缓存命中率高，则提高文件I/O的效率。  
  
4. **数据一致性和可靠性**：  
- 数据在内核空间的缓冲区中进行处理，确保了在复制到用户空间之前，数据经历了内核的处理，例如缓存写回、同步等，从而提高了数据的一致性和可靠性。  
  
****************************  
  
直接 I/O 存在的原因：  
- 对于一些大文件，用传统的缓存 I/O 方式，缓存命中率可能不高，反而会因为需要两次数据拷贝而增加开销，降低 I/O 的效率。  
- 直接 I/O 对于需要大规模数据传输的应用非常有用，例如大文件的读写、数据库操作等。在这些场景下，减少数据拷贝的开销可以显著提高性能。  
- 直接 I/O 在需要高性能计算和大数据处理的领域得到广泛应用。这些场景对 I/O 性能的要求非常高，而直接 I/O 提供了更高效的数据传输方式。  
- 直接 I/O 可以降低内存占用，因为数据不需要首先缓存在内核缓冲区中。对于大文件或大量数据的操作，这可以减少内存的使用。  
- 直接 I/O 不依赖于文件系统缓存，因此可以避免一些与缓存相关的问题，如缓存污染或不一致性。  
- 异步 I/O 只支持直接 I/O。  
  
# 内存分配算法  
> [Partition Allocation Methods in Memory Management](https://www.geeksforgeeks.org/partition-allocation-methods-in-memory-management/)  
> [OS Memory Allocation Q & A #2](https://www.tutorialspoint.com/operating_system/os_memory_allocation_qa2.htm)  
  
  
内存分配算法是操作系统中用于管理和分配可用内存空间的策略。以下是几种常见的内存分配算法：  
  
1. 首次适应算法（First Fit Algorithm）：从内存空闲链表的起始位置开始查找，找到第一个能够满足需求的空闲块，并将进程分配到该块。  
  
2. 最佳适应算法（Best Fit Algorithm）：在内存空闲链表中查找最小且能够满足需求的空闲块，并将进程分配到该块。该算法可以最小化内存碎片的产生，但搜索过程较慢。  
  
3. 最坏适应算法（Worst Fit Algorithm）：在内存空闲链表中查找最大的空闲块，并将进程分配到该块。该算法可以减少外部碎片，但可能导致内存利用率降低。  
  
4. 循环首次适应算法（Next Fit Algorithm）：类似于首次适应算法，但从上一次分配的位置开始查找，避免每次都从头开始搜索。  
  
5. 分区算法（Partitioning Algorithms）：将内存划分为固定大小的分区，每个分区可以容纳一个进程。分区算法包括等大小分区算法（Fixed-size Partitioning）和可变大小分区算法（Variable-size Partitioning），如动态分区分配算法（Dynamic Partitioning）和伙伴系统算法（Buddy System）等。  
  
伙伴系统是一种常用的内存分配算法，用于动态分配可变大小的内存块。它通过将可用内存空间划分为大小为2的幂次的块，以满足不同大小的内存需求，并通过合并和拆分操作来管理内存块。  
  
下面是伙伴系统内存分配算法的基本原理：  
  
- 初始化内存：首先，将整个可用的内存空间划分为最大块大小的内存块。通常情况下，内存大小是2的幂次，比如1KB、2KB、4KB等。  
  
- 分配内存：当有进程需要分配内存时，伙伴系统会根据进程的内存需求，找到最小的合适内存块。如果找到的内存块大小正好满足需求，则直接分配给进程。否则，会将该内存块一分为二，形成两个相等大小的伙伴块。  
  
- 合并操作：当进程释放内存时，伙伴系统会检查该内存块的伙伴块是否也是空闲的。如果是，则将两个伙伴块合并成一个更大的内存块，并递归地继续合并相邻的空闲块，直到无法再合并为止。  
