现代操作系统 第二章 进程和线程

# 进程
-  A process is just an instance of an executing program, including the current values of the program counter, registers, and variables.

- 前台进程
foreground processes, that is, processes that interact with (human) users and perform work for them. 
- 后台进程
Processes that stay in the background to handle some activity such as email, Web pages, news, printing, and so on are called daemons.

Four principal events cause processes to be created:
1. System initialization.
2. Execution of a process-creation system call by a running process.
3. A user request to create a new process.
4. Initiation of a batch job.

## Process Creation
In UNIX, there is only one system call to create a new process: fork. 

This call creates an exact clone of the calling process. 
After the fork, the two processes, the parent and the child, have the same memory image, the same environment strings, and the same open files. 

After a process is created, the parent and child have their own distinct address spaces. 

Some UNIX implementations share the **program text** between the two since that cannot be modified.

Alternatively, the child may share all of the parent’s memory, but in that case the memory is shared **copy-on-write**, which means that whenever either of the two wants to modify part of the memory, that chunk of memory is explicitly copied first to make sure the modification occurs in a private memory area. 

No writable memory is shared. 

## Process Termination
通常造成进程终止的条件有：
1. Normal exit (voluntary).
such as execute **exit** system call
2. Error exit (voluntary).
3. Fatal error (involuntary).
4. Killed by another process (involuntary).

## Process Hierarchies
In some systems, when a process creates another process, the parent process and child process continue to be associated in certain ways. 
The child process can itself create more processes, forming a process hierarchy.

## Process States
![](img/2023-10-02-11-20-36.png)


***************

> [Process state](https://en.wikipedia.org/wiki/Process_state)
> [States of a Process in Operating Systems](https://www.geeksforgeeks.org/states-of-a-process-in-operating-systems/)

> These distinct states may not be recognized as such by the operating system kernel. However, they are a useful abstraction for the understanding of processes.

### Created
- The process awaits admission to the "ready" state.
- Admission will be approved or delayed by a long-term, or admission, scheduler.

- 需要申请一个空白的 PCB，完成资源分配

### Ready
- A "ready" or "waiting" process has been loaded into main memory and is awaiting execution on a CPU. 

### Running
- A process moves into the running state when it is chosen for execution.
- There is at most one running process per CPU or core.
- A process can run in either of the two modes, namely kernel mode or user mode.

### Blocked
- A process transitions to a blocked state when it cannot carry on without an external change in state or event occurring.
- For example, a process may block on a call to an I/O device such as a printer, if the printer is not available.
- Processes also commonly block when they require user input, or require access to a critical section which must be executed [atomically](https://stackoverflow.com/questions/15054086/what-does-atomic-mean-in-programming).
- Such critical sections are protected using a synchronization object such as a semaphore or mutex.


### Terminated
- A process may be terminated, either from the "running" state by completing its execution or by explicitly being killed.
- The terminated process remains in the process table as a zombie process until its parent process calls the `wait` system call to read its exit status, at which point the process is removed from the process table, finally ending the process's lifetime.
- If the parent failes to call `wait`, this process continues to consume the process table entry (concretely the process identifier or PID), and causes a resource leak.


### Additional process states
> Two additional states are available for processes in systems that support virtual memory. 
> In both of these states, processes are "stored" on secondary memory (typically a hard disk).

- 进程被**挂起**（suspended）且**不会被调度**。
A process **stops** as a result of receiving a **SIGSTOP**, **SIGTSTP**, **SIGTTIN**, or **SIGTTOU** signal, 
and it **remains** **stopped** **until** it receives a **SIGCONT** signal, at which point it becomes running again.

- 按 `Ctrl z` 可以让进程处于 suspended 的状态，相当于发送信号 `SIGSTOP` 


#### Swapped out and waiting
- suspended and waiting
- The process is removed from main memory and placed on external storage by the scheduler.
- It may be swapped back into the waiting state.
#### Swapped out and blocked
- suspended and blocked
- The process is both blocked and swapped out
- It may be swapped back in again under the same circumstances as a swapped out and waiting process.


## Implementation of Processes
To implement the process model, the operating system maintains a table (an array of structures), called the process table, with one entry per process. (Some authors call these entries process control blocks.) 

This entry contains important information about the process’state, including its program counter, stack pointer,
memory allocation, the status of its open files, its accounting and scheduling information, and everything else about the process that must be saved when the process is switched from running to ready or blocked state so that it can be restarted later as if it had never been stopped.

![](img/2023-10-02-11-53-24.png)

# 线程
Processes are used to group resources together; threads are the entities scheduled for execution on the CPU.
进程是资源分配的基本单位
线程是 CPU 调度的基本单位

## 为什么需要线程
The main reason for having threads is that in many applications, multiple activities are going on at once. 
Some of these may block from time to time. 
By decomposing such an application into multiple sequential threads that run in quasi-parallel, the programming model becomes simpler.

A second argument for having threads is that since they are lighter weight than processes, they are easier (i.e., faster) to create and destroy than processes.

Threads yield no performance gain when all of them are CPU bound, but when there is substantial computing and also substantial I/O, having threads allows these activities to overlap, thus speeding up the application.

Threads are useful on systems with multiple CPUs, where real parallelism is possible


## 线程之间共享和独占的资源

![](img/2023-10-07-11-03-42.png)

 
Each thread has its own stack. 
Each thread’s stack contains one frame for each procedure called but not yet returned from.
This frame contains the procedure’s local variables and the return address to use when the procedure call has finished.


## 线程状态
- `thread_create`
When multithreading is present, processes usually start with a single thread present. 
This thread has the ability to create new threads by calling a library procedure such as `thread_create`.

Sometimes threads are hierarchical, with a parent-child relationship, 
but often no such relationship exists, with all threads being equal.

With or without a hierarchical relationship, the creating thread is usually returned a thread identifier that names the new thread.

- `thread_exit`
When a thread has finished its work, it can exit by calling a library procedure, say, `thread_exit`.

- thread_yield
`thread_yield` allows a thread to voluntarily give up the CPU to let another thread run.

## 多线程需要考虑的问题
1. If the parent process has multiple threads, should the child also have
them? 

If not, the process may not function properly, since all of them may be essential.

If the child process gets as many threads as the parent, what happens if a thread in the parent was blocked on a read call, say, from the keyboard

2.  What happens if one thread closes a file while another one is still read-
ing from it?

## POSIX Threads
To make it possible to write portable threaded programs, IEEE has defined a standard for threads in IEEE standard 1003.1c. 
The threads package it defines is called Pthreads. Most UNIX systems support it. 

![](img/2023-10-07-11-21-39.png)


## Implementing Threads 
> [线程的实现](https://xiaolincoding.com/os/4_process/process_base.html#线程的实现)

There are two main places to implement threads: user space and the kernel.

![](img/2023-10-07-11-39-09.png)

### Implementing Threads in User Space
When threads are managed in user space, each process needs its own private thread table to keep track of the threads in that process. 

This table is analogous to the kernel’s process table, except that it keeps track only of the per-thread properties, such as each thread’s program counter, stack pointer, registers, state, and so forth. 

The thread table is managed by the run-time system.

优点：
- 用户空间实现的线程进行上下文切换相对进程切换很快
- They allow each process to have its own customized scheduling algorithm. 

缺点：
- First among these is the problem of how blocking system calls are implemented.
One of the main goals of having threads in the first place was to allow each one to use blocking calls, 
but to prevent one blocked thread from affecting the others.

一个线程发起系统调用而阻塞会影响进程中的其他线程

- Another alternative is available in the event that it is possible to tell in advance if a call will block.

- Somewhat analogous to the problem of blocking system calls is the problem of page faults.
If a thread causes a page fault, the kernel, unaware of even the existence of threads, 
naturally blocks the entire process until the disk I/O is complete, even though other threads might be runnable.01.

- Another problem with user-level thread packages is that if a thread starts running, no other thread in that process will ever run unless the first thread voluntarily gives up the CPU. 

一个线程开始运行后，除非它主动交出 CPU 使用权，否则进程中的其他线程无法运行，因为用户态线程不能由操作系统调度
进程内部可以可以自己做一些调度规则

### Implementing Threads in Kernel Space
The kernel has a thread table that keeps track of all the threads in the system. 

When a thread wants to create a new thread or destroy an existing thread, 
it makes a kernel call, which then does the creation or destruction by updating the kernel thread table.

优点：
一个线程因为系统调用阻塞，不会影响进程中的其他线程
CPU 根据线程调度

缺点：
操作系统维护线程表、线程控制块 TCB
线程的创建、切换和终止都是通过系统调用，系统开销大

### Hybrid Implementation
一个应用程序可以同时用内核线程和用户线程，一个内核线程可以对应多个用户线程，数量由用户定义

![](img/2023-10-07-13-06-24.png)

In this model, each kernel-level thread has some set of user-level threads that take turns using it.
只有内核线程能被 CPU 调度

## Scheduler Activations
调度程序激活，是操作系统调度和执行线程的一种机制，旨在改进多线程程序的性能和可伸缩性

The goals of the scheduler activation work are to mimic the functionality of kernel threads, but with the better performance and greater flexibility usually associated with threads packages implemented in user space. 

When scheduler activations are used, the kernel assigns a certain number of virtual processors to each process and lets the (user-space) run-time system allocate threads to processors.

virtual processors 的数量可以根据需要动态增加和减少

当一个线程阻塞时，内核通过 upcall 通知用户空间的 run-time system，passing as parameters on the stack the number of the thread in question and a description of the event that occurred. 
然后 run-time system 重新调度一个新线程执行，因此避免一个线程阻塞而影响进程中其他线程

### upcall
上调用，用于将控制权从低级别的执行环境传递到高级别的执行环境。

上调用通常发生在操作系统内核和应用程序之间，或者在较低级别的软件组件和较高级别的软件组件之间。当发生某种事件或需要上层处理时，低级别的环境会发起一个上调用，将控制权传递给较高级别的环境，以便执行相应的处理逻辑。

上调用的目的是向上层组件或环境报告事件、请求服务或通知状态变化等。高级别的执行环境可以是应用程序的用户空间、应用程序的库、操作系统的内核等。

上调用的过程通常涉及一些协议和机制，以确保正确的传递控制权和数据。这可能包括上下文切换、参数传递、状态保存和恢复等操作。

上调用在操作系统和应用程序之间起到了桥梁的作用，允许不同层次的软件组件进行交互和协作。它提供了一种灵活的机制，使得低级别的组件能够通知和请求高级别的组件，从而实现功能扩展、事件处理和系统管理等操作。

总之，上调用是一种将控制权从低级别的执行环境传递到高级别的执行环境的机制，用于报告事件、请求服务或通知状态变化等。它在计算机系统中起到了重要的协作和交互的作用。

# Interprocess Communication
> [5.2 进程间有哪些通信方式？](https://xiaolincoding.com/os/4_process/process_commu.html)

进程间通信，IPC

进程间通信需要面临下面三个问题：
- 进程将怎么传递信息
传递信息在线程之间比较好处理，因为线程之间共享地址空间
- 处理多个进程竞争同一个资源的情况
- 多个进程之间运行可能有一定依赖关系，需要合适的顺序
如进程 B 可能需要进程 A 执行完成才能执行，该问题也称为同步（synchronization）问题


第二个和第三个问题在多线程通信同样需要解决

## Race Conditions
多个进程一起访问共享资源可能导致竞争条件

To see how interprocess communication works in practice, let us now consider a simple but common example: a print spooler. 
When a process wants to print a file, it enters the file name in a special spooler directory. 
Another process, the printer daemon, periodically checks to see if there are any files to be printed, and if there are, it prints them and then removes their names from the directory.

Imagine that our spooler directory has a very large number of slots, numbered 0, 1, 2, ..., each one capable of holding a file name. 

Also imagine that there are two shared variables, `out`, which points to the next file to be printed, and `in`, which points to the next free slot in the directory. 
These two variables might well be kept in a two-word file available to all processes. 

![](img/2023-10-07-18-48-06.png)

见上图所示，可能发生下面情况：
1. 进程 A 读变量 `in` 的值后保存该值 7 到局部变量 `next_free_slot`中
2. 然后该进程所占的时钟周期用完，CPU 调度到进程 B 来运行
3. 进程 B 同样获取变量 `in` 的值，即 7 
4. 接着进程 B 保存一个文件名到 slot 7 然后将 `in` 的值设置为 8，接着做其他事
5. 等 CPU 又调度到进程 A 运行时，由于之前进程 A 保存了 `in` 的值为 7，因此将要写入的文件名写到 slot 7 中覆盖掉进程 B 写入的内容， 然后进程 A 将 `in` 设置为 8
6. 最后进程 B 写入的文件名将不会被打印

上面这种多个进程读写共享资源，最终的结果取决于谁在何时准确运行的情况，结果存在不确定性（indeterminate）叫做竞争条件，即 race conditons。

## Critical Regions
为了避免竞争条件，需要避免多个进程同时读写共享资源，即 mutual exclusion，互斥

临界区，即 critical region，指一段代码或代码块，其中包含对共享资源的访问

如果能避免多个进程同时进入临界区，则能避免竞争

![](img/2023-10-07-19-09-55.png)


## 互斥的实现
> [互斥与同步的实现和使用](https://xiaolincoding.com/os/4_process/multithread_sync.html#互斥与同步的实现和使用)


### Disabling Interrupts
从之前例子看出，竞争出现在于一个进程访问共享资源时，操作到一半被打断，然后另一个进程访问该资源，
因此第一种方案是不让第一个进程访问时被中断

该方案不可取：
1. 让用户进程有权限关闭中断是很危险的行为，如果该进程一直占用 CPU 资源呢？
2. 即使让内核来禁止中断，也无法解决多核的情况，多核仍会发生竞争

### 锁
假设有一个共享变量 `turn`，初始该变量的值为 0，如果某个进程需要进入临界区，先查看该变量的值，
该值为 0 则表示临界区没有其他进程访问，因此该进程将变量设置为 1 获取锁，然后进入临界区

该方案需要查看变量和设置变量的操作不能中断，否则仍会发生竞争，即一个进程查看变量为 0，但还未设置其值为 1就被中断，
接着另一个变量获取锁，切换到第一个进程时，该进程根据之前获取的 `turn` 值 0，也进入临界区且将变量设置为 1

上述问题可以通过 TSL 指令解决

#### TSL 指令
- Test and Set Lock
- 该指令测试和设置锁，是一组原子操作指令，即要么全部执行完成，要么不执行
It reads the contents of the memory word lock into register RX and then stores a nonzero value at the memory address lock. 
The CPU executing the TSL instruction locks the memory bus to prohibit other CPUs from accessing memory until it is done.

#### 自旋锁 spin lock
当一个进程获取不到锁时，一直循环测试锁是否已被释放，等待获取锁，也被称为忙等待锁
A lock that uses busy waiting is called a spin lock.

这种忙等待可能浪费 CPU 时间
可以用 sleep 来阻塞而非循环测试浪费 CPU 时间，然后通过 wakeup 唤醒


### 生产者-消费者问题
- The producer-consumer problem

加入两个进程共享一段缓冲区，其中一个进程为生产者，向缓冲区中放入数据；另一个进程从缓冲区取出数据，为消费者
这里假设一个生产者和一个消费者

当缓冲区满了时，生产者停止生成，进入 sleep 状态，直到消费者消费数据后将其唤醒
当缓冲区空了时，消费者停止消费，进入 sleep 状态，直到生成者生成数据后将其唤醒

用一个共享变量 `count` 来表示缓冲区的数据的大小

如果消费者准备消费时，获取 `count` 的值为 0，将其保存，结果此时调度器将此进程停止，调度到生产者进程
生产者进程生产一个数据，此时 `count` 的值为 1，
生产者意识到之前缓冲区无数据，认为消费者处于 sleep 状态，因此调用 wakeup 将消费者唤醒
消费者此时并没有处于 sleep 状态，因此不会被 wakeup 唤醒
当调度器调度到消费者进程时，消费者根据之前保存的 `count` 值 0，进入 sleep 状态，等待被唤醒
而生产者继续生产，不会再调用 wakeup 将消费者唤醒，直到缓冲区满，此时生产者也进入 sleep 状态，等待消费者消费后将其唤醒
最终生成者和消费者都进入 sleep 状态

上述问题也是两个进程访问共享资源 `count` 而发生的竞争情况造成的

解决方案：
增加一个 wakeup waiting bit，当一个进程处于 awake 状态而收到 wakeup 信号时，将该位置 1，
如果之后该进程准备进入 sleep 状态，则检查 wakeup waiting bit，如果为 1，则进程仍保持 awake 状态，然后将该位清零

但如果有多个进程，一个 wakeup waiting bit 不够用，需要多个多个位

### 信号量 Semaphores
A semaphore could have the value 0, indicating that no wakeups were saved, or some positive value if one or more wakeups were pending.

信号量位一个整数变量，其值表示可用的资源数量

信号量有两个原子操作，即 down 和 up，或者 sleep 和 wakeup
- down
当一个进程要访问共享资源时，检查信号量的值是否大于 0，是则将值减 1，然后继续访问资源；
如果信号量为 0，则将进程进入 sleep 状态，等待其他进程释放资源
- up
当一个进程使用完共享资源后，执行 up 操作，将信号量的值加 1
如果此时有其他进程处于 sleep 状态在等待该信号量资源，则选择其中一个唤醒，使其可以执行 down 操作

down 和 up 操作最初也叫 Proberen (try) and Verhogen (raise, make higher) 操作，简写为 P 和 V。

### Mutexex
互斥锁，一种特殊的二进制信号量，只有两种状态：unlocked 和 locked
通常用 0 表示 unlocked，非零的其他整数表示 locked

### Futexes
futex, fast user space mutex
用户空间锁，提供高性能用户空间互斥锁，它的设计目标是在用户空间中实现互斥锁，避免频繁地陷入内核态，从而提高锁的性能和效率。

A futex consists of two parts: a kernel service and a user library. 

The kernel service provides a ‘‘wait queue’’ that allows multiple processes to wait on a lock. 
They will not run, unless the kernel explicitly unblocks them.

当一个进程（或线程）需要获取锁时，尝试在用户空间获取锁，如果锁是可用状态，则成功获取锁；
否则，futex library 利用系统调用将当前线程放入内核的 wait queue中

当持有锁的线程释放锁后，查看是否内核的 wait queue 中是否有进程处于阻塞状态，有则通知操作系统唤醒等待的进程

### Mutexex in Pthreads

![](img/2023-10-07-21-43-57.png)

In addition to mutexes, Pthreads offers a second synchronization mechanism: condition variables. 
Mutexes are good for allowing or blocking access to a critical region. 
Condition variables allow threads to block due to some condition not being met. 

![](img/2023-10-07-21-45-13.png)


### Monitors
monitor（监视器）是一种同步机制，用于管理共享资源的访问和保证多线程或多进程之间的正确同步。
它提供了一种结构化的方式来实现并发控制和互斥访问。

A monitor is a collection of procedures, variables, and data structures that are all grouped together in a special kind
of module or package. 

Processes may call the procedures in a monitor whenever they want to, but they cannot directly access the monitor’s internal data structures from procedures declared outside the monitor. 

Monitors have an important property that makes them useful for achieving mutual exclusion: **only one process can be active in a monitor at any instant**.

### Message Passing
消息传递，一种在并发计算和分布式系统中用于通信和同步的编程模型。
它是一种基于消息的通信方式，其中进程或线程通过发送和接收消息来进行相互之间的通信和数据交换。

在消息传递模型中，各个进程或线程是相对独立的实体，它们通过发送和接收消息来进行通信，而不是共享内存空间。
消息是一种信息单元，包含要传递的数据和相关的元数据。
发送者将消息发送到特定的目标，接收者则从指定来源接收消息。

消息传递使用两个原语，send 和 receive，是系统调用
```bash
send(destination, &message);
receive(source, &message);
```

消息传递方案面临的问题：
1. 消息传递可能发生在不同的机器之间，通过网络传输，因此有丢失风险
发送消息后，接收者需要发送 acknowledgement 消息表示已经接收到消息

但也有可能接收者已经收到消息，但确认消息丢失，因此发送者重新发送消息，
此时接收者需要能区分重新发送的消息和新消息，可以通过给消息设置序号来区分

2. how processes are named
Message systems also have to deal with the question of how processes are named, so that the process specified in a send or receive call is unambiguous.

3. 认证 Authentication
客户端能认证服务端的身份

4. 同一个机器中不同进程消息传递性能低
Copying messages from one process to another is always slower than doing a semaphore operation or entering a monitor

### Barriers
barrier（屏障）是一种同步原语，用于确保多个线程或进程在某个点上达到同步，然后同时继续执行。
Barrier通常用于将多个并发任务或线程分成多个阶段，并在每个阶段结束时等待所有任务或线程完成，然后再进行下一个阶段的执行。

Some applications are divided into phases and have the rule that no process may proceed into the next phase
until all processes are ready to proceed to the next phase.

![](img/2023-10-07-22-14-48.png)

### Avoiding Locks: Read-Copy-UPdate
Read-Copy-Update (RCU) 是一种并发编程中的技术，用于实现高效的读操作和低冲突的写操作。
它通常应用于多核系统中，以提高并发性能和减少锁竞争。

RCU 的核心思想是通过复制数据，并保留旧版本的数据（称为“读者”），以允许并发的读操作，而不会阻塞写操作。
当需要进行写操作时，RCU 使用一种延迟策略，确保所有读操作完成后才进行更新。

RCU 的主要步骤如下：

1. 读操作：当进行读操作时，读者线程可以直接访问共享的数据结构，而不需要加锁。这允许多个读者同时访问数据，提高了并发性能。

2. 写操作：当进行写操作时，RCU 使用一种延迟策略。它首先复制当前的数据结构，创建一个新的版本，然后将新版本的指针替换旧版本的指针。这样，新的读者可以读取新版本的数据，而旧的读者仍然可以访问旧版本的数据。

3. 延迟和回收：RCU 使用延迟策略确保所有读操作都完成后再回收旧版本的数据。这通常涉及到一些机制，如引用计数、等待队列等，以确保没有读者正在访问旧版本的数据，然后才进行回收操作。

RCU 的优点包括：

- 高并发性：RCU 允许多个读者并发地访问共享数据，从而提高了读操作的性能和并发性能。
- 低写冲突：由于读操作不需要加锁，写操作可以在不阻塞读操作的情况下进行。这减少了锁竞争和冲突的概率，提高了系统的吞吐量。
- 低开销：相比于传统的锁机制，RCU 的开销相对较低，因为读操作不需要获取锁。

然而，RCU 也存在一些限制和注意事项，如对数据结构的特定要求、需要额外的内存空间来存储旧版本的数据、写操作的延迟等。

![](img/2023-10-07-22-26-26.png)

# Scheduling





